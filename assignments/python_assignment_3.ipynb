{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Assignment 5: Using post-LASSO on large spatial data\n",
    "# This assignment will give you a real (active) research topic that I've discussed a little bit in class:\n",
    "# predicting carbon storage as a function of high-resolution gridded data. In the class google drive for data\n",
    "# you will see a new assignment_5 folder we will use.\n",
    "\n",
    "# This assignment will have you use the automated variable selection approach within LASSO to deal with a common situation\n",
    "# in regressions on raster-stacks: we have so much data everything is significant but will lead to massive overfitting.\n",
    "# The basic approach used here will involve reading in 2d rasters, flattening them into a 1d column ready to add\n",
    "# to a dataframe shaped object, which we will use as our X matrix.\n",
    "\n",
    "# You may use either Jupyter notebooks or a .py script file. If you turn in a jupyter file, save it as a PDF like before.\n",
    "# if you durn in a script file, turn in both the .py file AND a PDF/word doc of whatever is outputted when you run it.\n",
    "# The directions below assume you are turning in a Word document but you are still free to choose what to turn in.\n",
    "\n",
    "# PRELIMINARY STEP: Import these libraries.\n",
    "# Load libraries\n",
    "import numpy as np\n",
    "import os\n",
    "from osgeo import gdal\n",
    "from sklearn.linear_model import Lasso\n",
    "from matplotlib import pyplot as plt\n",
    "from statsmodels.api import OLS\n",
    "\n",
    "\n",
    "# Step 1: download the data and assign a relative path to the soyo_tile directory in that assignment directory.\n",
    "# Here, I actually want you to use the exact path below so that it works on my machine too. It\n",
    "# is your task to ensure your script runs in the right location and the data is stored in the right\n",
    "# location that this relative path works.\n",
    "\n",
    "data_dir = '../../Data/assignment_5/soyo_tile'\n",
    "\n",
    "\n",
    "# Step 2: assign each of the raster paths in the directory to a dictionary for later use. I've included\n",
    "# most of the code (so you don't have to waste your time typing), but add in the three missing\n",
    "# paths.\n",
    "\n",
    "raster_paths = {}\n",
    "raster_paths['agb_observed_baccini_2000_30m'] = os.path.join(data_dir, \"agb_observed_baccini_2000_30m.tif\")\n",
    "\n",
    "raster_paths['CRFVOL_M_sl1_250m'] = os.path.join(data_dir, \"CRFVOL_M_sl1_250m.tif\")\n",
    "raster_paths['HISTPR_250m'] = os.path.join(data_dir, \"HISTPR_250m.tif\")\n",
    "raster_paths['OCDENS_M_sl1_250m'] = os.path.join(data_dir, \"OCDENS_M_sl1_250m.tif\")\n",
    "raster_paths['PHIHOX_M_sl1_250m'] = os.path.join(data_dir, \"PHIHOX_M_sl1_250m.tif\")\n",
    "raster_paths['roughness_30s'] = os.path.join(data_dir, \"roughness_30s.tif\")\n",
    "raster_paths['SLGWRB_250m'] = os.path.join(data_dir, \"SLGWRB_250m.tif\")\n",
    "raster_paths['SLTPPT_M_sl1_250m'] = os.path.join(data_dir, \"SLTPPT_M_sl1_250m.tif\")\n",
    "raster_paths['SNDPPT_M_sl1_250m'] = os.path.join(data_dir, \"SNDPPT_M_sl1_250m.tif\")\n",
    "raster_paths['terrain_ruggedness_index_30s'] = os.path.join(data_dir, \"terrain_ruggedness_index_30s.tif\")\n",
    "raster_paths['TEXMHT_M_sl1_250m'] = os.path.join(data_dir, \"TEXMHT_M_sl1_250m.tif\")\n",
    "raster_paths['wc2.0_bio_30s_01'] = os.path.join(data_dir, \"wc2.0_bio_30s_01.tif\")\n",
    "raster_paths['alt_30s'] = os.path.join(data_dir, \"alt_30s.tif\")\n",
    "raster_paths['AWCh1_M_sl1_250m'] = os.path.join(data_dir, \"AWCh1_M_sl1_250m.tif\")\n",
    "raster_paths['BDRICM_M_250m'] = os.path.join(data_dir, \"BDRICM_M_250m.tif\")\n",
    "raster_paths['BDRLOG_M_250m'] = os.path.join(data_dir, \"BDRLOG_M_250m.tif\")\n",
    "raster_paths['BLDFIE_M_sl1_250m'] = os.path.join(data_dir, \"BLDFIE_M_sl1_250m.tif\")\n",
    "\n",
    "\n",
    "# Step 3: Our dependent variable will be 30 meter observations of carbon storage from\n",
    "# Baccini et al. (unpublished, but soon to be published) data. The label I assigned in the dictionary\n",
    "# above was agb_observed_baccini_2000_30m for this variable.\n",
    "# Use gdal.Open, GetRasterBand(1) and ReadAsArray() to read this geotiff as a numpy file\n",
    "# Side note:\n",
    "# If you get an error like: \"ERROR 4: This is a BigTIFF file.  BigTIFF is not supported by this version of GDAL and libtiff.\"\n",
    "# make sure you have conda installed gdal from the CONDA FORGE  using the command \"conda install gdal -c conda-forge\" option.\n",
    "\n",
    "\n",
    "# Step 4, Create an empty numpy array (or full of zeros) of the right shape to house all our raster data.\n",
    "# A very CPU-efficient way of arranging a stack of 2d rasters (which would be 3d once stacked up), is\n",
    "# to flatten each 2d raster into a longer 1d array. This will go into our X matrix.\n",
    "# In order to create the right sized X matrix, first get the n_obs and n_vars by inspecting\n",
    "# the dependent variable raster and the dictionary of inputs above.\n",
    "# Note that the n_vars should be the number of INDEPENDENT and DEPENDENT variables\n",
    "# report in your WORD DOCUMENT the size of the data_array you created.\n",
    "\n",
    "\n",
    "# Step 5, Iterate through the dictionary and load each raster as a 2d array, flatten it to 1d\n",
    "# using the .flatten() method in numpy. Assign this 1d array to the correct column\n",
    "# of the data array. By convention, the depvar will be the first column.\n",
    "\n",
    "# Hint, assuming you have arranged your X array in the correct way, it should have observations (pixels)\n",
    "# as rows and variables as cols. Given that each flattened array is for one variable and is as long as there\n",
    "# are rows, a convenient way of assigning it would be to use numpy slice notation, potentially similar to:\n",
    "# data_array[:, column_index_integer].\n",
    "# The first colon just denotes the whole row and the column index is an integer you could create pointing to the right row.\n",
    "# Some incomplete code to get you started is below.\n",
    "\n",
    "for name, path in raster_paths.items():\n",
    "    print('Loading', path)\n",
    "    flattened_raster_array = band.ReadAsArray().flatten()\n",
    "    data_array[:, col_index] = flattened_raster_array\n",
    "    feature_names.append(name)\n",
    "\n",
    "\n",
    "# Step 6, extract the first array row of the data_array and assign it to y. Assign the rest to X.\n",
    "\n",
    "\n",
    "# Step 7, split the X and y into testing and training data such that\n",
    "# the training data is the first million pixels and the testing data is the next 200,000\n",
    "# Do this using numpy slice notation on the X and y variables you created.\n",
    "\n",
    "\n",
    "# Step 8 (optional but useful). To make the code run faster, we are going to\n",
    "# use every 10th pixel. We can easily get this via numpy slicing again, using\n",
    "# x_train[::10] to get every 10th pixel.\n",
    "\n",
    "\n",
    "# Step 9, create a Lasso object (using the default penalty term alpha)\n",
    "# and fit it to the training data. Create\n",
    "# and print out a vector of predicted carbon values. Also print out the score\n",
    "# using the lasso object's .score() method on the TESTING data.\n",
    "# Add the fitted lasso score to your WORD DOCUMENT.\n",
    "\n",
    "\n",
    "# Step 10, optional and just for fun. To view how our projections LOOK, we can\n",
    "# create a predicted matrix on the whole X, reshape it back into the\n",
    "# original 2d shape and look at it. You can compare this to the input array\n",
    "# to visualize how it looks. Note that this will only work if you name your objects\n",
    "# like mine.\n",
    "\n",
    "# full_prediction = fitted_lasso.predict(X)\n",
    "# prediction_2d = full_prediction.reshape(2000, 2000)\n",
    "# plt.imshow(prediction_2d)\n",
    "# plt.show()\n",
    "#\n",
    "# plt.imshow(array)\n",
    "# plt.show()\n",
    "\n",
    "\n",
    "# Step 11, Create a list of 30 alphas using np.logspace(-1, 3, 30). Using a for loop\n",
    "# iterate over those alphas and run the Lasso model like above, but using the alpha values\n",
    "# in the loop. Print out the fit score at each step. Using matplotlib, plot how this\n",
    "# value changes as alpha changes. Finally, extract the best alpha of the bunch.\n",
    "# Put the plot of alphas and their scores in your WORD DOCUMENT along with the value\n",
    "# of the optimal alpha.\n",
    "\n",
    "alphas = np.logspace(-1, 3, 30)\n",
    "\n",
    "\n",
    "# Step 12, rerun the lasso with that best value and identify all of the coefficiencts\n",
    "# that were \"selected\" ie had non-zero values. Save these coefficient indices and labels\n",
    "# to a list.\n",
    "\n",
    "\n",
    "# Step 13, Using Statsmodels, run an OLS version on the selected variables.\n",
    "# Copy and paste your resulting OLS.summary() table to your WORD DOCUMENT. In addition\n",
    "# add to your WORD DOCUMENT a description of how this result is better than a vanilla OLS.\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3.10.6 ('8222env1')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  },
  "vscode": {
   "interpreter": {
    "hash": "0db313e0ad7b6749a6d098fb61fddaded88cbd823278030b75fa0893942c8f77"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
